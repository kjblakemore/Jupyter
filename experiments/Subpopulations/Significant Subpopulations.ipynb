{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman Correlations of Dependent Variable & Independent Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "DATA_DIR_NAME = '../../data/'\n",
    "\n",
    "MAX_CARDINALITY = 4000\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data Set\n",
    "1. Impute missing values - median for discrete variables, mean for continuous\n",
    "2. Separate dependent variable\n",
    "3. Bin continuous variables\n",
    "4. One-hot encode independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(dependent_col_name, pdf, continuous_columns, discrete_columns, categorical_columns):\n",
    "  \n",
    "    # Drop columns with high cardinality\n",
    "    max_cardinality = min(MAX_CARDINALITY, pdf.shape[0])\n",
    "    pdf = pdf.drop([col for col in categorical_columns if pdf[col].nunique() >= max_cardinality], axis=1)\n",
    "    \n",
    "    # Impute missing discrete values\n",
    "    for col in discrete_columns:\n",
    "        pdf[col] = pd.to_numeric(pdf[col], downcast = 'integer')\n",
    "        pdf[col] = pdf[col].fillna(pdf[col].median())\n",
    "        \n",
    "    # Impute missing continuous values\n",
    "    for col in continuous_columns:\n",
    "        pdf[col] = pd.to_numeric(pdf[col])\n",
    "        pdf[col] = pdf[col].fillna(pdf[col].mean())\n",
    "        \n",
    "    # separate dependent variable from independent ones\n",
    "    if dependent_col_name in discrete_columns: discrete_columns.remove(dependent_col_name)\n",
    "    if dependent_col_name in continuous_columns: continuous_columns.remove(dependent_col_name)\n",
    "        \n",
    "    dependent_col = pdf[dependent_col_name]\n",
    "    pdf = pdf.drop([dependent_col_name], axis=1)\n",
    "    \n",
    "    # bin continuous variables\n",
    "    for col in continuous_columns:\n",
    "        edges = np.linspace(pdf[col].min(), pdf[col].max(), num=10).tolist()\n",
    "        pdf[col], edges = pd.cut(pdf[col], edges, retbins = True)\n",
    "        \n",
    "    # one hot encode variables\n",
    "    for col in pdf:\n",
    "        col_dummies = pd.get_dummies(pdf[col], prefix=col, prefix_sep='=')\n",
    "        pdf = pdf.drop(col, axis=1)\n",
    "        pdf = pd.concat([pdf, col_dummies], axis=1)  \n",
    "\n",
    "    return dependent_col, pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Correlations and Rank Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRELATION_COLUMNS = ['value', 'correlation', 'p_value', 'count', 'score']\n",
    "\n",
    "def calculate_correlations(dependent_col, pdf):\n",
    "    correlations_pdf = pd.DataFrame(index=range(pdf.shape[1]), columns = CORRELATION_COLUMNS)\n",
    "    idx = 0\n",
    "    \n",
    "    for col_name, col in pdf.iteritems():\n",
    "        rho, p_value = stats.spearmanr(dependent_col, col)\n",
    "        count = col.sum()\n",
    "        row = {'value': col_name, 'correlation': rho, 'p_value': p_value, 'count': count}\n",
    "        correlations_pdf.loc[idx] = row\n",
    "        \n",
    "        # print every 100 calculations   \n",
    "        if(idx % 100 == 0): print(idx, row)\n",
    "        idx += 1\n",
    "     \n",
    "    correlations_pdf['score'] = abs(correlations_pdf['correlation'])* (1 - correlations_pdf['p_value'])\n",
    "    correlations_pdf = correlations_pdf.sort_values(by=['score'], ascending=False)\n",
    "    return correlations_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - Hospital Readmissions with Overall Rating as Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DATA_SET_NAME = 'hospital_readmissions'\n",
    "pdf = pd.read_csv(DATA_DIR_NAME + DATA_SET_NAME + '.csv', encoding='latin-1')\n",
    "print(pdf.shape)\n",
    "display(pdf.head())\n",
    "print(pdf.nunique())\n",
    "\n",
    "pdf = pdf.replace({'Below the national average': '1',\n",
    "                   'Same as the national average': '2', \n",
    "                   'Above the national average': '3',\n",
    "                   'Not Available': '',\n",
    "                   'Too Few to Report': '',\n",
    "                   'Results are not available for this reporting period': ''\n",
    "                  }\n",
    "                 )\n",
    "\n",
    "continuous_columns = ['Excess Readmission Ratio',\n",
    "                      'Expected Readmission Rate',\n",
    "                      'Predicted Readmission Rate',\n",
    "                      'Number of Discharges',                   \n",
    "                      'Number of Readmissions'\n",
    "                     ]\n",
    "\n",
    "discrete_columns = ['Effectiveness of care national comparison',\n",
    "                    'Efficient use of medical imaging national comparison',\n",
    "                    'Hospital overall rating',\n",
    "                    'Mortality national comparison',\n",
    "                    'Patient experience national comparison',\n",
    "                    'Readmission national comparison',\n",
    "                    'Safety of care national comparison',\n",
    "                    'Timeliness of care national comparison'\n",
    "                   ]       \n",
    "\n",
    "categorical_columns = ['Address',\n",
    "                       'City', \n",
    "                       'County Name', \n",
    "                       'Effectiveness of care national comparison footnote',\n",
    "                       'Efficient use of medical imaging national comparison footnote',\n",
    "                       'Emergency Services',\n",
    "                       'End Date',\n",
    "                       'Footnote',\n",
    "                       'Hospital Name',\n",
    "                       'Hospital Ownership',\n",
    "                       'Hospital Type',\n",
    "                       'Hospital overall rating footnote',\n",
    "                       'Measure Name',\n",
    "                       'Meets criteria for meaningful use of EHRs',\n",
    "                       'Mortality national comparison footnote',\n",
    "                       'Patient experience national comparison footnote',\n",
    "                       'Phone Number',\n",
    "                       'Provider ID',\n",
    "                       'Provider Number',\n",
    "                       'Readmission national comparison footnote',\n",
    "                       'Safety of care national comparison footnote',\n",
    "                       'Start Date',\n",
    "                       'State',\n",
    "                       'Timeliness of care national comparison footnote',\n",
    "                       'ZIP Code'\n",
    "                      ]\n",
    " \n",
    "dependent_col_name = 'Hospital overall rating'  \n",
    "    \n",
    "dependent_col, prepped_pdf = data_preparation(dependent_col_name, pdf, continuous_columns, discrete_columns, categorical_columns)\n",
    "print(prepped_pdf.shape)\n",
    "\n",
    "correlations_pdf = calculate_correlations(dependent_col, prepped_pdf)\n",
    "display(correlations_pdf.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - Titanic Data Set with Survived as Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME = 'titanic'\n",
    "\n",
    "pdf = pd.read_csv(DATA_DIR_NAME + DATA_SET_NAME + '.csv', encoding='latin-1')\n",
    "print(pdf.shape)\n",
    "display(pdf.head())\n",
    "print(pdf.nunique())\n",
    "\n",
    "continuous_columns = ['Age',\n",
    "                      'Fare',\n",
    "                      'Parch',\n",
    "                      'SibSp'\n",
    "                     ]\n",
    "\n",
    "discrete_columns = ['Pclass',\n",
    "                    'Survived'\n",
    "                   ]\n",
    "    \n",
    "categorical_columns = ['Cabin',\n",
    "                       'Embarked',\n",
    "                       'Name',\n",
    "                       'PassengerId',\n",
    "                       'Sex',\n",
    "                       'Ticket'\n",
    "                      ]\n",
    "\n",
    "drop_columns = ['Cabin',\n",
    "                'Name',\n",
    "                'PassengerId',\n",
    "                'Ticket'\n",
    "               ]\n",
    "\n",
    "dependent_col_name = 'Survived'  \n",
    "    \n",
    "dependent_col, prepped_pdf = data_preparation(dependent_col_name, pdf, continuous_columns, discrete_columns, categorical_columns)\n",
    "print(prepped_pdf.shape)\n",
    "\n",
    "correlations_pdf = calculate_correlations(dependent_col, prepped_pdf)\n",
    "display(correlations_pdf.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
